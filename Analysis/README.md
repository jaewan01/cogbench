# LLM Analysis Scripts

This repository contains various Python scripts that were used for the analysis in our paper. However, the main emphasis of our work is the benchmark itself. We encourage users to create their own analysis. CogBench provide easy access to the experiments' data in their '../Experiments/`<chosen experiment>`/scores_data.csv' file and the LLM information in './data/llm_features.csv', making it easy to compare them to other LLMs or human data.

All scripts merge the information from 'llm_features.csv' with 'scores_data.csv' for the chosen experiment and run analysis.

## Folder Structure

- **Python Scripts**: All Python scripts are located in the root directory. These include `umap_plot.py`, `phenotype_comp.py`, `perf_regression.py`, `regression.py`, `cot_sb.py`, `cot_sb_legend.py`, `radar_plot.py`, and `utils.py`.

- **Data**: The 'data' directory contains the 'llm_features.csv' file, which stores the information about the LLM engines and their features. This file needs to be updated when analyzing new LLMs.

- **Plots**: The 'plots' directory contains the output plots generated by the scripts.

## Python Scripts

Most scripts have `agents_to_include` or `agents_to_exclude` variables, `experiments` dictionnary to be chosen, and `interest` variables for either performance or behavior analysis. Apart from these, almost nothing has to be specified and the default scripts should map to the figures in the paper.

### Experiments Dictionary

This dictionary is structured with the keys being the names of the experiments and the values being the metrics of interest for each experiment.

For example:

```python
experiments = {
    "experiment1": ["metric1", "metric2"],
    "experiment2": ["metric3", "metric4"],
    # ...
}

```
By default, the experiments dictionary is set to the experiments and metrics used in our paper. These default metrics were chosen for their relevance to the experiments conducted. If you want to analyze different experiments or metrics, you can modify this dictionary accordingly. 


To add the analysis of a new LLM, you need to:

1. Add its feature information in 'llm_features.csv'.
2. Ensure the relevant experiments are run and the metrics are stored in the experiment's 'scores_data.csv' in the '../Experiments/`<chosen experiment>`' directory.
3. Specify the LLM to be analyzed in the analysis script.

### Scripts
- `utils.py`: Contains most of the functions used in the other scripts.

- `phenotype_comp.py`: Generates bar plots comparing all the performance or behavior of different LLMs engines across various tasks. This represents Figure 2 in the paper. 

- `umap_plot.py`: Creates a UMAP plot of either the behavioral or performance metrics of all ran LLMs across all experiments. This represents Figure 3.A in the paper. 

- `perf_regression.py` and `regression.py`: Perform multi-level regression analysis on performance and behavioral scores respectively of various LLM engines across multiple experiments. This represents Figure 4 in the paper.

- `cot_sb.py`: Compares the performance of baseline models with those using Chain of Thought (CoT) and Step Back (SB) prompting techniques.
- `cot_sb_legend.py`: Generates the legend for the comparison plot created by `cot_sb.py`. This represents Figure 5 in the paper.

- `radar_plot.py`: Performs radar plot analysis on specified metrics.

## Contributions

Contributions are welcome! We are particularly interested in better analysis methods and more accurate information about LLMs and their features (fill in the './data/llm_features.csv' file).

## Remarks

1. Context length should be the context length of training data (e.g codellama has a context length of 100 but trained on 16k)
2. Difficult to find the fined-tuned models datasize. 
    - For LoRA, they say 3k long questions in the [paper](https://arxiv.org/pdf/2309.12307.pdf) which are up to 32k. So we estimate the datasize to be 3k * 32k = 96M tokens ~= 100M tokens
    - Codellama [paper](https://arxiv.org/pdf/2308.12950.pdf) says that it has 500B tokens for the dataset